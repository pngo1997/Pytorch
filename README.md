# ðŸ”„ Recurrent Neural Networks (RNN) with PyTorch  

## ðŸ“œ Overview  
This project explores **Recurrent Neural Networks (RNNs)** using **PyTorch**, and processes sequential data.

ðŸ“Œ **Key Concepts Covered**:  
- **Recurrent Neural Networks (RNNs)**  
- **Handling Sequential Data**  
- **Training an RNN in PyTorch**  
- **Backpropagation Through Time (BPTT)**  
- **Evaluating RNN Predictions**  

## ðŸš€ Implementation Details  

### **Step 1: Data Preprocessing**  
âœ… **Load and preprocess sequential data**.  
âœ… **Convert data into tensors for PyTorch**.  

### **Step 2: Building the RNN Model**  
âœ… **Define an RNN model** using `torch.nn.RNN()`.  
âœ… **Customize hidden layers and activation functions**.  
âœ… **Initialize weights and biases**.  

### **Step 3: Training the RNN**  
âœ… **Define loss function and optimizer**.  
âœ… **Train the model over multiple epochs**.  
âœ… **Backpropagate errors using PyTorchâ€™s autograd system**.  

### **Step 4: Model Evaluation & Visualization**  
âœ… **Evaluate performance on test sequences**.  
âœ… **Visualize predictions vs. actual data**.  
- Overall, there is overfitting here since there is a significant gap between the training and validation accuracy. The training accuracy quickly reaches near 100%, indicating that the model is effectively learning the training data; however, the validation accuracy remains significantly lower compared to the training accuracy and is relatively flat throughout the epochs. On the other hand, the training loss decreases rapidly and approaches zero, which corresponds with the high training accuracy. Meanwhile, the validation loss fluctuates and slowly increases over time. This indicates that the model learns the training set extremely well but fails to perform effectively on unseen data.
![Graph](https://github.com/pngo1997/Images/blob/main/Pytorch.png)  

## ðŸ“Œ Summary  
âœ… Implemented an RNN from scratch using PyTorch.

âœ… Trained the model on sequential data.  

âœ… Visualized model performance and predictions.
